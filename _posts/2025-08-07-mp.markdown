---
layout: post
title: "CNN与其反向传播"
author: "Brahmsky"
date: 2025-08-7 18:00:00 +0800  # 文章发布日期和时间（可选，但推荐）
categories: [深度学习, CNN]      # 文章分类（可以是多个）
tags: [CV, CNN, 深度学习] # 文章标签（可以是多个）
catalog: true                       # 是否显示目录 (根据你的主题)
header-style: text                  # 头部样式 (根据你的主题)
---

卷积神经网络的三个基本概念：局部感受野，共享权重，池化。
输入层乘以一个权重矩阵得到隐藏层到过程，叫特征映射。这玩意的核心思想就是，共享权重，共享偏置。这一个特征映射定义的共享权重矩阵就叫卷积核，或滤波器。

一个卷积层由多个不同的特征映射组成——这些特征映射定义的卷积核生成的是**一个“隐藏层”的多个“特征图”（Feature Maps）**。一个卷积层有**多个卷积核**，每个卷积核学习不同的特征，每个卷积核的**深度等于输入层的channel数**，如3\*3\*3（如果是灰度图，卷积核的深度就是1）。这一个卷积核其实包含了三个3\*3的二维滤波器，这一个卷积核的最终输出是三个滤波器通道卷积结果做一个求和，然后**加上一个偏置项**，加上偏置以后，整个**丢进一个激活函数**（可以是sigmoid或者ReLU），最终得到一个**单通道特征图**。每个卷积核的输出按深度堆叠在一起，形成输出，称为该卷积层的隐藏层，作为特征表示。

比如说输入定义成H * W * C(channel)或者叫做深度，可以是一个28\*28\*1，然后卷积核是5\*5\*1，有20个特征映射。一个卷积核有25个权重项+1个偏置项=26个参数，总共就有25\*26=520个参数定义一个卷积层。（这里的HWC表示仅仅限于这一段）

卷积核的深度，与其输入层相同。

卷积层之后，就是池化层。池化层的基本单元获取卷积层的浓缩的激活值，比如说最大池化可以是一个2\*2区域的最大的激活值。池化后由24\*24就得到了12\*12个神经元（Stride = 池化窗口大小）。卷鸡层64个通道，池化以后的层数还是不变的，就是对输出的每一个通道做了一个max pooling。

常见的池化方式，除了max pooling还有平均池化，L2池化等等，但是用的都不如max pooling多

在一般的表示中（这里采用pytorch里头的表示），一个4D图像张量的格式为NCHW，即（batchsize，channel，height，width）。

一个典型的CNN的末端结构：-> **最后的池化层** (输出形状: 3 x 12 x 12) -> **展平层 (Flatten Layer)** (输出形状: 432) -> **全连接层1** (输入: 432, 输出: N个神经元，如128) -> **激活函数 (ReLU)** -> **全连接层2 (输出层)** (输入: N, 输出: 类别数，如10) -> **Softmax激活函数**。
这个flatten操作，一般的实践是：
按照池化层的通道深度由浅到深展开，每一个通道都是行优先展开，如果数据格式是 `(C, H, W)`，那么展平就是**先通道、再行、再列**（channel → height → width 顺序）。

这个时候就有大聪明会问了：如果前面的卷积层和池化层是负责特征提取，既然前面卷积核的参数也是可变的，我们学习到的特征每次在输入展平后的线性层的时候都是一个不同的分布，那么在每个epoch的训练都会产生不同分布的基础上去优化最后一整的权重的和偏置有什么意义呢？
其实，这是一个协同进化的过程，如果卷积层学会了提取更有利于分类的特征，那么全连接层就能更容易地学习到正确的决策边界。咋就这么说呢？早期卷积层往往会学习到一些相对**稳定的、通用的**底层视觉模式，比如边缘、角点、颜色梯度等。这些基础特征在训练过程中可能不会发生剧烈的变化。
池化层（特别是最大池化）具有一定的平移不变性。即使特征图上的激活位置稍微移动，池化操作也可能产生相似的输出，这在一定程度上**稳定了传递给全连接层的输入**。最重要的，**批归一化**显著减小了**Internal Covariate Shift（内部协变量偏移**）的问题。即使前几层的参数发生变化，批归一化也能保证输入到后续层的分布相对稳定。

这个时候还会有大聪明会问了：这里的展平难道不会丢失一些空间信息吗，还是说最后经过池化以后的特征已经是高度浓缩的了，压扁成一个一维向量也不会损失什么了？
包会损失的！只是到了这里，空间位置信息的重要性已经大大降低了，因为前面的卷积核池化已经将信息提取的足够抽象和具有空间位置不变性了，特征本身的存在性变得更加重要了。
-**全局平均池化 (Global Average Pooling, GAP)**: 像ResNet、Inception等现代网络结构，很多都用**全局平均池化**替代了Flatten操作。它将每个 HxW 的特征图直接池化成一个单一的数值（取整个图的平均值）。如果最后有64个特征图，就会得到一个长度为64的向量。这种方法被认为更加优雅，因为它：
1.保留了“一个特征图对应一个输出值”的直观联系。
2.极大地减少了从卷积层到分类层之间的参数数量，防止了过拟合。
3.在结构上强制让特征图对应到类别，增强了模型的可解释性。

好了，接下来是又一个核心挑战：实现CNN的反向传播。
首先贴出来传统MLP的BP四大方程:

1.$\frac{\partial C}{\partial z_j^l} = \frac{\partial C}{\partial a_j^l} \cdot \frac{\partial a_j^l}{\partial z_j^l} = \frac{\partial C}{\partial a_j^l} \cdot \sigma'(z_j^l)$。写成矩阵形式，就是$\delta^L = \nabla_a C \odot \sigma'(z^L)$。**(BP1，输出层误差方程)**

2.想要计算$\delta^l = \frac{\partial C}{\partial z^l}$，已知$\delta^{l+1} = \frac{\partial C}{\partial z^{l+1}}$，所以$\frac{\partial C}{\partial z^l_j} = \sum_k \frac{\partial C}{\partial z^{l+1}_k} \frac{\partial z^{l+1}_k}{\partial z^l_j}$
又$z^{l+1}_k = \sum_p w_{kp}^{l+1} \sigma(z_p^l) + b_k^{l+1}$，对第j项带权输入求偏导只有第j项会被保留，于是$\frac{\partial z^{l+1}_k}{\partial z^l_j} = w_{kj}^{l+1} \sigma'(z_j^l)$。把结果带回$\delta^l_j = \frac{\partial C}{\partial z^l_j} = \sum_k \delta^{l+1}_k \cdot w_{kj}^{l+1} \sigma'(z_j^l) = \left( \sum_k w_{kj}^{l+1} \delta^{l+1}_k \right) \sigma'(z_j^l)$。根据矩阵乘法，$\delta^l = ((W^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)$。**(BP2，误差反向传播方程)**

3.由于$z_j^l = \sum_k w_{jk}^l a_k^{l-1} + b_j^l$，于是$\frac{\partial C}{\partial b_j^l} = \delta_j^l$。**(BP3，偏置梯度方程)**

4.$\frac{\partial C}{\partial w_{jk}^l} = a_k^{l-1} \delta_j^l$。**(BP4，权重梯度方程)**

**网络结构:**

- **输入 (a⁰)**: 1 x 28 x 28
- **卷积层 (Layer 1)**: 3个 1x5x5 卷积核, 步幅1.
    - 带权输入: z¹, 激活输出: a¹ (形状 3 x 24 x 24)
- **池化层 (Layer 2)**: 2x2 最大池化, 步幅2.
    - 输出: a² (形状 3 x 12 x 12)
- **展平层 (Layer 3)**:
    - 输出: a³ (形状 432 x 1 的向量)
- **输出层 (Layer L=4)**: 全连接层，10个神经元。
    - 带权输入: z⁴, 激活输出: a⁴ (形状 10 x 1 的向量)
- **损失函数**: Softmax + 交叉熵
---
![[Pasted image 20250808111249.png]]
就一个batch来说，利用BP1，$δ_4=a_4−Y$，这个是输出层的误差。
BP4/3，输出层权重梯度$\frac{\partial C}{\partial W^4} = \delta^4 (a^3)^T$，偏置梯度$\frac{\partial C}{\partial b^4} = \delta^4$。
BP2，输出层的误差传导到前一层展平层，$\delta^3 = ((W^4)^T \delta^4) \odot \sigma'(z^3)$。这个$\sigma'(z^3)$，$z^3$作为展平层的输出，因为这一层本来就没有什么激活函数，认为activation(x) = x，于是这个导数值就是1。于是公式简化为$\delta^3 = (W^4)^T \delta^4$

我们得到了展平层的432个误差值以后，就可以按照展平的逆过程进行**重塑(reshape)**，把所有误差丢到前一层，变成一个3\*12\*12的，和池化层形状相同的误差矩阵$δ^2$。也就是说$\delta^3$和$\delta^2$只有一个形状上的差别。

然后接下来才是卷积神经网络特有的部分：unpooling，**将池化层的梯度还原到池化前的特征图尺寸**。对每个通道逐个unpool。这个步骤中，我们的目光还是首先集中在池化层上。上一步已经得到了池化层的每个通道的误差矩阵，这一步首先盯住这个矩阵。（类似于一个scatter操作：根据索引，把数据填到指定位置）
如果前馈传播采用的是max pooling，就采用这个方式：把池化误差$\delta^2[c,i,j]$，放到正向传播时一个block中数值最大的那个的位置，其他位置置0。这样就填充起来一个和卷积层形状相同的3\*24\*24的稀疏矩阵，稀疏梯度图。
如果前馈传播采用的是average pooling，把池化误差$\delta^2[c,i,j]$除以4填进哪一个block的4个位置中（因为正向传播时一个block的池化结果就是对四个数值取平均，于是误差也均匀分配回去）。这样就填充起来一个一个和卷积层形状相同的3\*24\*24的一个“一块一块”的矩阵，均匀分布的梯度图。
至于Global Average Pooling, GAP，全局平均池化，就是平均池化的最极端版本，生成一个全体数字都相同的一个误差矩阵。

有小朋友就问了，有没有严谨的推导？有的兄弟有的，核心依然只有一个：多元链式法则。
	令$a^l$为卷积层的输出特征图（$C \times H_{in} \times W_{in}$），$a^{l+1}$是池化层的输出($C \times H_{out} \times W_{out}$)，已知池化层的误差$\delta^{l+1}$，目标是计算$\delta^l$。对于输入图 $a^l$ 上任意一个神经元 $(c, i, j)$，其梯度为：$$\frac{\partial C}{\partial a^l_{c, i, j}} = \sum_{c'} \sum_{m} \sum_{n} \frac{\partial C}{\partial a^{l+1}_{c', m, n}} \frac{\partial a^{l+1}_{c', m, n}}{\partial a^l_{c, i, j}}$$池化是每个通道独立进行，通道 $c$ 的输出 $a^{l+1}_{c, \cdot, \cdot}$ 只依赖于通道 $c$ 的输入 $a^l_{c, \cdot, \cdot}$。所以直接把第一个sigma和砍掉（好无聊的一步）：$$\frac{\partial C}{\partial a^l_{c, i, j}} = \sum_{m=1}^{H_{out}} \sum_{n=1}^{W_{out}} \frac{\partial C}{\partial a^{l+1}_{c, m, n}} \frac{\partial a^{l+1}_{c, m, n}}{\partial a^l_{c, i, j}}$$
	现在，只需分析偏导数项 $\frac{\partial a^{l+1}_{c, m, n}}{\partial a^l_{c, i, j}}$ 对不同池化方式的意义。问的是，输入的某个点 $(i, j)$ 对输出的某个点 $(m, n)$ 的影响有多大？我们令 $\mathcal{R}_{m,n}$ 是输入图 $a^l$ 中对应输出点 $(m,n)$ 的那个池化窗口区域。如果 $(i,j)$ 不在区域 $\mathcal{R}_{m,n}$ 内，它对 $a^{l+1}_{c, m, n}$ 就没有影响，所以偏导数为0。那么如果在这个区域里面呢？
	最大池化中的情形中，若 $a^l_{c, i, j}$ 是该区域的最大值，则 $a^{l+1}_{c, m, n} = a^l_{c, i, j}$，所以 $\frac{\partial a^{l+1}_{c, m, n}}{\partial a^l_{c, i, j}} = 1$。若 $a^l_{c, i, j}$ 不是该区域的最大值，那么微小改变 $a^l_{c, i, j}$ 的值并不会改变最大值的结果，所以 $\frac{\partial a^{l+1}_{c, m, n}}{\partial a^l_{c, i, j}} = 0$。代入上式，$$\frac{\partial C}{\partial a^l_{c, i, j}} =
\begin{cases}
\frac{\partial C}{\partial a^{l+1}_{c, m, n}} & \text{如果 } (i,j) \text{ 是区域 } \mathcal{R}_{m,n} \text{ 的最大值位置} \\
0 & \text{否则}
\end{cases}$$，显而易见。
	平均池化的情形中，$a^{l+1}_{c, m, n} = \frac{1}{|\mathcal{R}_{m,n}|} \sum_{(i',j') \in \mathcal{R}_{m,n}} a^l_{c, i', j'}$，所以$\frac{\partial a^{l+1}_{c, m, n}}{\partial a^l_{c, i, j}} = \frac{1}{|\mathcal{R}_{m,n}|}$ 。一个输入点 $(i,j)$ 只属于一个池化区域 $\mathcal{R}_{m,n}$（假设步幅与窗口大小一致，不重叠），求和自动消失$$\frac{\partial C}{\partial a^l_{c, i, j}} = \frac{\partial C}{\partial a^{l+1}_{c, m, n}} \cdot \frac{1}{|\mathcal{R}_{m,n}|}$$，数学意义也很明显。

**回顾整个过程，我们把$δ_4$，经过BP2的MLP反向传播得到了$δ_3$，然后reshape成了$\delta^2$，然后或是scatter或是平均，的把$\delta^2$给unpool成$\delta^1$。误差传播的过程就结束了。**

现在，我们得到了误差矩阵$\delta^1$（形状 `3x24x24`）。接下来就是通过这个误差矩阵，和卷积层的输入 $a^0$ (形状 `1x28x28`)计算$\frac{\partial C}{\partial b^1}$和$\frac{\partial C}{\partial W^1}$，也就是一个 `3x1` 的偏置梯度向量+3个 `1x5x5` 卷积核的梯度。
先是CNN版的BP3。三个卷积核，需要求三个偏置。
揪出来某一层的一个偏置值，$$\frac{\partial C}{\partial b^1_c} = \sum_{i=1}^{24} \sum_{j=1}^{24} \frac{\partial C}{\partial z^1_{c, i, j}} \cdot \frac{\partial z^1_{c, i, j}}{\partial b^1_c}$$，前向传播时第一个偏置$b_1^1$被**加到**了第一个 24x24 特征图的**每一个**元素上，很明显$\frac{\partial z^1_{c, i, j}}{\partial b^1_c}$这一项按照ij展开以后每一项都为1。所以，$$\frac{\partial C}{\partial b^1_c} = \sum_{i=1}^{24} \sum_{j=1}^{24} (\delta^1_{c, i, j})$$。第一个偏置的梯度，因为被广播到了整个输出图，反向传播结果就是第一个 `24x24` **误差图上所有误差的总和**。剩余两个通道上的偏置梯度也是一样的计算方法。

---
再是CNN版BP4。三个卷积核，也是三个梯度矩阵。
从权重矩阵中揪出来某个权重值 $W^1_{c, u, v}$（第 $c$ 个卷积核在 $(u,v)$ 位置的权重），梯度是$$\frac{\partial C}{\partial W^1_{c, u, v}} = \sum_{i=1}^{24} \sum_{j=1}^{24} \frac{\partial C}{\partial z^1_{c, i, j}} \cdot \frac{\partial z^1_{c, i, j}}{\partial W^1_{c, u, v}}$$。正向传播时，卷积操作的本质其实就是$$z^1_{c, i, j} = \sum_{u'=0}^{4} \sum_{v'=0}^{4} W^1_{c, u', v'} \cdot a^0_{i+u', j+v'} + b^1_c$$(表示隐藏层某个位置的加权输入等于局部感受野内的每个元素和卷积核做乘积后相加，类似hadamard积)，局部感受野内只有那个和我们关心的$W^1_{c, u, v}$所在位置重叠的元素，才会按照这个元素的数值作为比例放大w的误差。于是，$\frac{\partial z^1_{c, i, j}}{\partial W^1_{c, u, v}}=a^0_{i+u, j+v}$， $$\frac{\partial C}{\partial W^1_{c, u, v}} = \sum_{i=1}^{24} \sum_{j=1}^{24} \delta^1_{c, i, j} \cdot a^0_{i+u, j+v}$$。概念上不难理解，前向传播时，该权值的梯度就会按照其**随着卷积核的滑动所覆盖到的位置**（好比一支中性笔戳在5\*5正方形纸片的某个位置上，拖动正方形纸片让其轨迹铺满28\*28的输入图像，每个时刻中性笔下面都戳着某个输入$a^0$值，最后中性笔会画满一个24\*24的大正方形）的像素值进行比例放大。当我们关注另一个位置的权值，相当于中性笔换了一个地方戳在正方形纸片上，得到的就是另一个位置上的24\*24的大正方形。整个过程中隐藏层的误差矩阵是固定的，可以把**误差图 $\delta^1_c$** 当作一个在**输入图 $a^0$** 上滑动的卷积核。于是，形式可以非常简洁：$\frac{\partial C}{\partial W^1_c} = \text{conv}(a^0, \delta^1_c)$。
剩余两个通道做法一样。

这个网络过于简单了，误差传播到卷积层得到$\delta^1$后就结束了，没有触及到CNN反向传播的核心——CNN版的BP2。我们在掌握其他三个方程基础上，设想一个更加复杂的深度CNN，**卷积层数≥2**。这个时候就要考虑一个问题：误差信号如何跨过卷积层，利用该层的卷积核权重矩阵反向传播？

现在我们手握一个卷积层形状的 `δˡ⁺¹`，目标是计算一个形状为该卷积层输入形状的误差矩阵 `δˡ`。揪出来一个元素，依然是链式法则，正向传播时有$$\delta^l_{c, i, j} = \frac{\partial C}{\partial z^l_{c, i, j}} = \left( \sum_{c'} \sum_{m} \sum_{n} \frac{\partial C}{\partial z^{l+1}_{c', m, n}} \frac{\partial z^{l+1}_{c', m, n}}{\partial a^l_{c, i, j}} \right) \cdot \frac{\partial a^l_{c, i, j}}{\partial z^l_{c, i, j}}$$，所有复杂性都集中在中间的偏导数项 $\frac{\partial z^{l+1}_{c', m, n}}{\partial a^l_{c, i, j}}$ 上。它问的是：**前一层 `l` 的某个激活值 $a^l_{c, i, j}$，对后一层 `l+1` 的带权输入 $z^{l+1}_{c', m, n}$ 到底有多大的影响？** 输入图中(c, i, j)一个位置实际上是参与了输出图中所有通道的运算的，所以这里就不能砍掉c的sigma和了。考虑**误差向后一层传播时，点 (c, i, j) 在某个输出通道上的误差，是这个输出通道上踩到过它的卷积核“分核”在输出图上该通道计算结果的误差，按照踩到时核上对应权重加权求和的结果。每个输出通道上的误差加起来，得到点 (c, i, j) 对输出图的的总误差。** 
设想这个分核在该通道上**从上到下，从左到右**移动，也在输出图上从左到右、从上到下地产生输出结果。分核移动时，我们关注的那个点 (c, i, j) 在核上“可汗大点兵”的顺序则是**从下到上、从右到左**。该点在输出图上产生贡献的位置顺序，与贡献之权值在核上的移动顺序恰好相反。因此，在固定输出层的误差图，将后一层作为conv的输入时，依照链式法则，按误差图的二维坐标正向索引展开误差，对应产生该输出值的卷积核权重值的索引就是上下、左右全都颠倒的，等价于一个颠倒的卷积核在误差图上从上到下、从左到右地，按照全卷积的方式移动。上下左右都颠倒等价于平面内旋转180°。这个操作正式名叫**转置卷积**。
上面这个过程，是单个误差图通道与单个翻转卷积”分核“的操作。对于误差图的一个通道，**用这个通道分别与与其对应的那个卷积核每个翻转分核进行全卷积**，就将一个通道的输出误差复原到了形状为前一层通道数的输入误差。对误差图的每个通道都做一样的操作，把所有计算的到的输入的“分误差”叠加，就是整个输入误差图。$$BP2:\delta^l = \text{conv\_full}(\delta^{l+1}, \text{rot180}(W^{l+1})) \odot \sigma'(z^l)$$，这个公式是高纬度的抽象，可以来解释张量运算。

可以对比：

| 方程  | 全连接网络 (MLP)                                                | 卷积网络 (CNN)                                                                              | 核心思想类比            |
| --- | ---------------------------------------------------------- | --------------------------------------------------------------------------------------- | ----------------- |
| BP2 | $\delta^l = ((W^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)$ | $\delta^l = \text{conv\_full}(\delta^{l+1}, \text{rot180}(W^{l+1})) \odot \sigma'(z^l)$ | 矩阵转置 $\iff$ 转置卷积  |
| BP4 | $\frac{\partial C}{\partial W^l} = \delta^l (a^{l-1})^T$   | $\frac{\partial C}{\partial W^l} = \text{conv}(a^{l-1}, \delta^l)$                      | 外积 $\iff$ 卷积      |
| BP3 | $\frac{\partial C}{\partial b^l} = \delta^l$               | $\frac{\partial C}{\partial b^l} = \sum_{i,j} (\delta^l)_{i,j}$                         | 单个误差 $\iff$ 误差图求和 |

然后你会发现，“转置卷积”好像并不是说的卷积核转置了，明明是旋转180°啊？其实这个“转置”操作，“转置”的是一个与全卷积操作在线性代数上等价的一个大型稀疏矩阵。
任何前向的卷积操作都可以被表示为一次大型稀疏矩阵（卷积矩阵C）与展平的输入向量a的乘法。误差反向传播的核心，即计算前一层输入的梯度，在数学上等价于用这个卷积矩阵的**转置Cᵀ**去乘以误差向量δ。‘转置卷积’是该操作的**理论名称**，揭示其在线性代数中的本质是对偶（转置）关系；而‘用旋转180度的核进行全卷积’则是**算法实现**，它描述了我们为了计算效率，在代码层面实际执行的具体步骤。两者指向的是同一个数学运算，只是从不同的层面进行描述。
所以，本质上CNN和MLP都是线性变换+激活函数，只是矩阵长得不一样。MLP是稠密矩阵，CNN是具有特定结构的稀疏的**托普利茨矩阵**。也就是说，前面的所有CNN版的反向传播方程，也可以像MLP一样从纯矩阵运算的角度推导。

我们说，CNN在前面的层学到的是像素级别的初级的特征，在后面的层学到一些语义的、组合的、高级的特征，这一切的核心机制，就是**感受野 (Receptive Field) 的扩大**。随着层数的加深，每一个神经元的“感受野”在不断扩大，经过前面卷积层和池化层的浓缩以后，这一个神经元处理的小区域内的信息就成了从原始图像中某个大区域浓缩过来的，就可以为图中相距较远的特征建立联系，建立某个物体的宏观概念。

最后的问题：
为啥只对全连接层dropout？
首先，一个典型的CNN，虽然前面可能涉及到比较深的卷积层和池化层，但是参数共享极大缩减参数量，90%的参数都会集中在FC层。flatten以后的特征是丢失掉空间信息的，过拟合的含义就是死记硬背高级特征组合。而卷积层先天就对过拟合有很强的抵抗力，共享权重强制学习局部特征，同时，限制模型复杂度，也引入了平移不变性，池化降采样也增强了平移不变性，降维。
我们设想一种随机丢弃单个神经元（像素）的Dropout。在特征图中，相邻的像素通常具有很强的相关性，即使一个像素被丢弃了，它的信息也很容易从相邻像素那里“泄露”过来。要在卷积层上dropout，需要采用特殊方式，如drop通道或者drop矩形区域等等。

