---
layout: post
title: "传统RAG优化方法"
author: "Brahmsky"
date: 2025-09-11 18:00:00 +0800  # 文章发布日期和时间（可选，但推荐）
categories: [深度学习, PyTorch]      # 文章分类（可以是多个）
tags: [Transformer, 注意力机制, NLP] # 文章标签（可以是多个）
catalog: true                       # 是否显示目录 (根据你的主题)
header-style: text                  # 头部样式 (根据你的主题)
---

主要三个过程，输入，检索，生成，围绕这个三元组之间的相关性去展开RAG有效性评估。
![一种RAG评估体系](https://i-blog.csdnimg.cn/direct/f51c08b11f6e413db4f72993269f7672.png)

上下文相关性：检索出来的上下文和query的相关性。
忠实度：模型的生成是否基于上下文
答案相关性：模型的生成和query之间的相关性
上下文召回率：上下文与ground truth之间的相关性
答案正确性：字面意思，生成答案的正确性
答案完整性：答案是否像ground truth一样完整

还有一些指标，比如说命中率，top_k中多少比例包含了答案；平均倒数排名，第一个相关文档排在了多少位的倒数；归一化折损累计增益，文档的相关性和在列表中的排名的一个综合指标；答案的完整性，流畅性，连贯性，答案对检索到上下文的利用率；还有一些传统的指标比如说BLUE，然后还有利用大模型的评估方法等等。

这里比较主流的一个RAG的评估体系是LangSmith(`from langsmith.evaluation import evaluate, LangChainStringEvaluator`)，与LangChain无缝集成。评估需要准备一个测试集，然后也是一个简洁的`results = evaluate(……)`。

---

然后是生成环境中如何优化。外部非结构化数据的清洗和处理、文本分块、Query 的预处理、是不是每次 Query 都要进行检索、上下文信息的检索和排序能力、知识缓存等环节都会影响系统的性能。
### Pre-retrieval 之 Query 优化：
#### 1. **RAG Fusion**，检索增强生成融合

通过LLM把原始query扩展成多个查询变体Multi-Query，去并行检索出独立的、多个排序好的相关文档列表，通过**倒数排名融合 (Reciprocal Rank Fusion, RRF)**，将得到的多个文档列表根据排名合并成一个最终排序列表。为什么？因为同样的c个文档，其前后排序可以说直接决定回复质量。
![RAG Fusion](https://i-blog.csdnimg.cn/direct/096149a429884761a2d24e556bf10e37.png)
众所周知啊，transformer架构的模型，**位置偏差 (Positional Bias)、“中间迷失”问题 (Lost-in-the-Middle Problem)** 是很头疼的。实践中模型对于**位置信息**的敏感度很高，**开头和结尾**部分的信息关注度最高，而对放在中间部分的信息则很容易忽略或遗忘，所谓的首位效应和末位效应。

于是同样的c个文档，文本块不同的排名，会显著地影响恢复质量。所以最好的实现就是把最关键的文档排在第一位，次要放第二，根据相关程度依次往下排，形成一个`[核心答案 -> 重要补充 -> 相关背景 -> 次要细节]` 的顺序排列，还能对模型的上下文窗口截断有良好的鲁棒性，所谓在理论和工程实践中取得平衡。

那么这个RRF是如何实现的呢？首先它不管原始相似度，只管排名，认为列表排名比不同查询的相似度分数更重要。如果一个文档在**多个不同**的查询结果中都**排名靠前**，那么它很可能就是真正重要的核心文档。对于每个独立的查询列表中的每一个文档，其“重要性”就是其在所有列表中$1 / (k + rank)$ 之和，$$RRF score(d) = Σ\frac{1}{rk(d) + β}  
$$其中`rk(d)`为文档排名，`β`为平滑参数（通常取60，降低排名靠后文档的影响）。根据RRF score去做文档重排序，排名最靠前的文档作为最终的、高质量的上下文，连同原始问题一起，交给后续的操作(如重排序等)。这样，经过“多路包抄”和“智能筛选”，上下文已*更全面、噪声更少、核心信息更突出*，上下文质量和回复质量远超传统RAG。

比如说，top_k=5(一般比最终送入模型的文档数多)，三个multi_query查出来10个文档，就会根据这10个文档的RRF score排名，最终把4个(可以指定)最靠前的送入模型。这个算法的精妙之处在于，**奖励在不同视角下都表现出相关性的文档**，可能说某个在两次查询中都出现过的文档，虽然在各自的查询列表中排名都不靠前，但是加起来就比一个排名靠前但是只出现1次的文档分数更高。

除此以外，这个“多路包抄”的策略，本身也是一个很好的优化。garbage in，garbage out，即使用户的查询模棱两可，或有瑕疵，或者偏离主题，其他扩展的查询也有可能命中关键信息，甚至某个特定的新查询会意外地检索到一份原始查询无法触及、但却至关重要的文档。降低了对用户query的敏感度，覆盖更广泛，上下文更为核心，噪声更少。

实践证明，这个方法效果是非常好的，优于其他融合方法。

#### 2.**HyDE**，假设性文档嵌入，Hypothetical Document Embeddings

核心思路：以虚引实。用户query会首先送给一个Instruction-Following LLM，这个模型会生成一个可能的假设性文档，可能是虚构的，可能是事实错误的，但是这不重要。真正目的是在**潜在的形式、术语和内容上**去模仿一个真实的、高质量的答案，提高检索的相关性。系统根据这个假设文档的embedding去查找，然后后面的流程就一样了，找到的最相关的几条，然后重排序，和用户query一块打包送给LLM，LLM根据这些有事实依据的东西回复。
这个法子实现简单，不需要任何fine-tune，可以应用于任何知识库，零样本zero-shot。但是比较吃假设文档的质量，如果假设文档出现严重方向偏差、幻觉，自然会把查询引导到错误的方向。

有一个自然的想法就是把RAG-Fusion和HyDE拼在一块，一个query先去生成multi-query，然后再对每一个query生成假设文档，然后查询，RRF，rerank，但是这样逻辑链路更长、更复杂，调试和维护的难度增加，同时边际效益大幅减少。垂直领域可能会考虑这样的思路，标配用不着。

当然，哪怕RRF score排序了也好，HyDE也好，cross-attention reranker依然少不了。这可不是冗余。
### Post-retrieval 之 高级检索优化
#### 1.句子窗口检索
![句子窗口检索](https://i-blog.csdnimg.cn/direct/9add22c1eb74462c81ca67361e26899f.png)
传统RAG，一个chunk整个向量化。小文本，检索时精细度高，但是喂给LLM时缺上下文。大文本，上下文完整，但是稀释关键信息，检索时大海捞针。

句子窗口检索，以单句为最小单位，每一个中心句都去维护一个包含前后句子的上下文窗口作为metadata，向量化时**只对中心句向量化**，这样就实现了理论上的最小粒度的查询；但是提交给LLM的上下文是**包含了完整上下文窗口的元数据**，这个思路的精妙之处就在于**分离了检索单元和提供给LLM的上下文单元**，同时保证了小粒度的检索，和完整的上下文，实现也简单。

这个上下文窗口的大小，调起来也是有很多讲究，太小的话上下文不足，太大就会引入噪声。如果检索到的多个句子本身就相邻，送入LLM之前得对大量重复文本做去重或合并处理。

#### 2.上下文压缩
传统RAG，`Retriever -> [Long, Noisy Chunks] -> LLM`。

上下文压缩，`Retriever -> [Long, Noisy Chunks] -> Compressor -> [Short, Relevant Snippets] -> LLM`。LLM接受到经过处理后的干净的、精炼的上下文和原始问题。

主要思路是把喂给LLM的上下文**去噪，提取精华**，避免噪声、避免Lost in the Middle、解决模型上下文限制问题、节约token。抽取式压缩：**从原始文档中“抽取”相关的句子，拼接起来**，比如说可以采用一个小LLM，或者Cross-Encoder去提取相关度最高的句子；摘要式压缩：LLM自己读文档，自己总结一个回答问题的摘要。上下文可以达到高度浓缩、精炼，但是可能会引入新的幻觉，污染真实性，在事实准确性要求高的情景下慎用。

#### 3.分层索引检索
![分层索引检索](https://i-blog.csdnimg.cn/direct/05502c5c38484827a77a00e7f72a8d96.png)核心思路，不要把所有信息扁平的放在一个池子里，而是建立一个目录结构。分治策略。有点像建立索引时的聚类检索，同样是分层/分组的哲学，但是这个目录的划分依据是文档的显式的结构，自上而下结构化，保留单个文档内部的上下文和叙事流。

实现方式是，先分割成大的parent chunks，为每一个父块生成摘要，然后父块进一步分割成子块（如最常见的分割成单个句子），其中每个子块的元数据都包含父块的ID。检索时，首先是query和目录做比较，快速找到宏观部分，限定搜索范围，然后在几个父块之内精细查找子块。找到子块以后，把整个父块作为上下文喂给LLM。整个思路和聚类检索迷之相似，聚类也是先去找到一个聚类中心，然后再精细查找，都是粗粒度-->细粒度的两阶段查找策略。但是聚类这个是**数据驱动 (Data-Driven)，算法自动发现**，而分层索引是**逻辑驱动 (Logic-Driven) ，文档原生 (Document-Native)，人为定义或遵循文档原有结构，具有清晰的语义和逻辑意义**。天然保证上下文关系，控制粒度高。

首先，依然是很吃摘要质量。其次，适合处理结构化的长篇文档，处理结构零散的文档发挥不出优势。

#### 4.混合检索
混合混合，混合的是**向量检索baseline**(传统RAG语义相似度检索) **+** **BM25**（风格非常统计学的，关键字搜索—稀疏检索算法）。这个BM25，本质上一种词袋模型（Bag-of-Words），会根据三部分计算query和document的相关性，词频TF(经过一个非线性饱和优化，即用一个公式去平滑词频的增长，避免饱和)、逆文档频率IDF $log \frac {总文档数 - 包含该词的文档数 + 0.5} {包含该词的文档数 + 0.5}$
(词在所有文档中越稀有，分数越高)、文档长度Doc Length(用来调整TF)三者计算一个分数$\text{BM25}(D, Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{\text{TF}(q_i, D) \cdot (k_1 + 1)}{\text{TF}(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}$

两种方法搜出来的文档列表，做RRF(最常用)或者加权，得到一个重新排序的文档列表送给下游的流程。这种混合检索能显著提升检索的鲁棒性和相关性，对于**纯自然语言查询**和**包含专业、专有名词的模糊查询**，都能得心应手。

### 语义路由
带知识库的agent。在查询之前，由一个路由决定要干啥，是要直接和LLM对话，还是要RAG，RAG的话去哪一个知识库查。这个就进入ai agent那一套范畴了。

### 重排序
见[[18.reranker]]

其他方向的优化如：
1.多跳查询，分解子问题、执行第一跳、上下文注入与查询重构、执行第二跳、合成，用于处理因果分析、比较、关系推理。
2.Graph RAG，涉及到知识图谱。
3.Agentic RAG，或者在生成答案以后由一个meta bot评估质量等等。


有这么些个优化方法，自然可以搭配组合，形成pipeline。
1.基础增强型，高性价比。`Query -> Hybrid Search -> Re-ranker -> LLM`，大多数RAG的最佳起点
2.查询优化型，处理复杂问题。`Query -> RAG Fusion -> 混合检索 -> RRF -> (可选)重排序 -> LLM
3.精读型，处理长文档和高密度信息。`Query -> 检索 -> 句子窗口检索 (Sentence Window) -> 上下文压缩 (Compressor) -> LLM`
4.生产级pipeline。可以尽管融合，引入agent那一套，由一个语义路由决定去选择哪一套pipeline。`Query -> 语义路由 (Router) -> Retreieve/API call/function call -> [Pipeline A | Pipeline B | ... ]`，适用于那种大型企业智能助理，复杂度比较高。

